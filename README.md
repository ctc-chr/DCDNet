# DCDNet
Enhancing Action Recognition via Dynamic Cross-Frame Differential Modeling
## Abstract
In the realm of action recognition, the dynamic changes of key human body parts across consecutive frames encapsulate the core semantic information of actions. Traditional approaches often prioritize single-frame static features or perform simplistic temporal modeling, overlooking the capacity of multi-scale frame differences to effectively characterize nuanced local action details. This paper introduces DCDNet, an action recognition method grounded in dynamic cross-frame differences, designed to explicitly enhance spatiotemporal difference perception through a multi-branch temporal modeling architecture. By designing three parallel dilated convolution branches with distinct dilation rates along the temporal dimension, DCDNet captures action features across varying frame spans, yielding a multi-scale spatiotemporal difference map. Through hierarchical feature fusion, DCDNet achieves state-of-the-art performance on the HMDB51, UCF101, and INCLUDE datasets, with accuracy rates of 74.01\%, 92.99\%, and 92.94\%, respectively. Visualization results corroborate DCDNet's capability to precisely localize fine-grained action segments, such as punching trajectories and gesture transitions, thereby substantiating its advantages in decoupling spatiotemporal features and focusing on dynamic weights.

