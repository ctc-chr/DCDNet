# DCDNet
Enhancing Action Recognition via Dynamic Cross-Frame Differential Modeling<br>
（*This code is part of the manuscript submitted to International Journal of Machine Learning and Cybernetics*）
## Abstract
In the realm of action recognition, the dynamic changes of key human body parts across consecutive frames encapsulate the core semantic information of actions. Traditional approaches often prioritize single-frame static features or perform simplistic temporal modeling, overlooking the capacity of multi-scale frame differences to effectively characterize nuanced local action details. This paper introduces DCDNet, an action recognition method grounded in dynamic cross-frame differences, designed to explicitly enhance spatiotemporal difference perception through a multi-branch temporal modeling architecture. By designing three parallel dilated convolution branches with distinct dilation rates along the temporal dimension, DCDNet captures action features across varying frame spans, yielding a multi-scale spatiotemporal difference map. Through hierarchical feature fusion, DCDNet achieves state-of-the-art performance on the HMDB51, UCF101, and INCLUDE datasets, with accuracy rates of 74.01\%, 92.99\%, and 92.94\%, respectively. Visualization results corroborate DCDNet's capability to precisely localize fine-grained action segments, such as punching trajectories and gesture transitions, thereby substantiating its advantages in decoupling spatiotemporal features and focusing on dynamic weights.

## Guidelines
The latest DCD module is the TtS_new.py file in the modules folder. As a plug-and-play module, readers are welcome to insert it into their own models for further experimentation. At the same time, I have conducted DCD module experiments in multiple network models, which are saved in the lib folder for readers' reference.<br>
If you use this code, please cite our manuscript: [Enhancing Action Recognition via Dynamic Cross-Frame Differential Modeling]
